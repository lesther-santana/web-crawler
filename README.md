# Web Crawler

This project contains a simple web crawler built with Node.js. It's designed to recursively visit web pages within the same domain, starting from a base URL, and count the occurrences of each visited URL.

## Features

- Recursive web crawling within the same domain.
- Counts and tracks the occurrences of visited URLs.
- Generates reports based on crawled data.

## Getting Started

### Prerequisites

- Node.js (version specified in `.nvmrc`)

### Installation

1. Clone the repository or download the project files.
2. Navigate to the project directory and install dependencies:

```bash
npm install
```


### Run the crawler:

```bash
npm run start BASE_URL
```